{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ad59cde-3406-466a-9682-d692224a612c",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://laelgelcpublic.s3.sa-east-1.amazonaws.com/lael_50_years_narrow_white.png.no_years.400px_96dpi.png\" width=\"300\" alt=\"LAEL 50 years logo\">\n",
    "<h3>APPLIED LINGUISTICS GRADUATE PROGRAMME (LAEL)</h3>\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d718c-dbe1-4c9c-8a01-7672f03593de",
   "metadata": {},
   "source": [
    "# Getting started tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d613e4-05f2-4bc0-855d-ced0c86e712c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-05T03:27:50.597903Z",
     "iopub.status.busy": "2023-12-05T03:27:50.597579Z",
     "iopub.status.idle": "2023-12-05T03:28:34.549525Z",
     "shell.execute_reply": "2023-12-05T03:28:34.548714Z",
     "shell.execute_reply.started": "2023-12-05T03:27:50.597877Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18a8201240d4af2b9d50add46bd54c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tbody><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3</td><td>application_1701743246151_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-39-43.sa-east-1.compute.internal:20888/proxy/application_1701743246151_0004/\" class=\"emr-proxy-link j-3QTG6VLOJGE30 application_1701743246151_0004\" emr-resource=\"j-3QTG6VLOJGE30\n",
       "\" application-id=\"application_1701743246151_0004\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-37-185.sa-east-1.compute.internal:8042/node/containerlogs/container_1701743246151_0004_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def calculate_red_violations(data_source, output_uri):\n",
    "    \"\"\"\n",
    "    Processes sample food establishment inspection data and queries the data to find the top 10 establishments\n",
    "    with the most Red violations from 2006 to 2020.\n",
    "\n",
    "    :param data_source: The URI of your food establishment data CSV, such as 's3://DOC-EXAMPLE-BUCKET/food-establishment-data.csv'.\n",
    "    :param output_uri: The URI where output is written, such as 's3://DOC-EXAMPLE-BUCKET/restaurant_violation_results'.\n",
    "    \"\"\"\n",
    "    with SparkSession.builder.appName(\"Calculate Red Health Violations\").getOrCreate() as spark:\n",
    "        # Load the restaurant violation CSV data\n",
    "        if data_source is not None:\n",
    "            restaurants_df = spark.read.option(\"header\", \"true\").csv(data_source)\n",
    "\n",
    "        # Create an in-memory DataFrame to query\n",
    "        restaurants_df.createOrReplaceTempView(\"restaurant_violations\")\n",
    "\n",
    "        # Create a DataFrame of the top 10 restaurants with the most Red violations\n",
    "        top_red_violation_restaurants = spark.sql(\"\"\"SELECT name, count(*) AS total_red_violations \n",
    "          FROM restaurant_violations \n",
    "          WHERE violation_type = 'RED' \n",
    "          GROUP BY name \n",
    "          ORDER BY total_red_violations DESC LIMIT 10\"\"\")\n",
    "\n",
    "        # Write the results to the specified output URI\n",
    "        top_red_violation_restaurants.write.option(\"header\", \"true\").mode(\"overwrite\").csv(output_uri)\n",
    "\n",
    "# Set the values for data_source and output_uri\n",
    "data_source = 's3://gelcawsemr/food_establishment_data.csv'\n",
    "output_uri = 's3://gelcawsemr/restaurant_violation_results'\n",
    "\n",
    "# Call the function with the provided values\n",
    "calculate_red_violations(data_source, output_uri)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577bc20-e9f8-48f4-a715-081ad8d67c6a",
   "metadata": {},
   "source": [
    "# Programme to uncompress the archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acca8d4-9a42-493e-9171-ca397ac9d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import tarfile\n",
    "import bz2\n",
    "import os\n",
    "\n",
    "# Set up AWS credentials\n",
    "aws_access_key_id = 'YOUR_ACCESS_KEY_ID'\n",
    "aws_secret_access_key = 'YOUR_SECRET_ACCESS_KEY'\n",
    "region_name = 'YOUR_REGION_NAME'\n",
    "\n",
    "# Set up S3 client\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)\n",
    "\n",
    "# Set up EMR client\n",
    "emr = boto3.client('emr', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key, region_name=region_name)\n",
    "\n",
    "# Set up the job flow\n",
    "job_flow_id = 'YOUR_JOB_FLOW_ID'\n",
    "\n",
    "# Set up the source and destination S3 bucket names\n",
    "source_bucket_name = 'YOUR_SOURCE_BUCKET_NAME'\n",
    "destination_bucket_name = 'YOUR_DESTINATION_BUCKET_NAME'\n",
    "\n",
    "# Set up the S3 bucket and file paths\n",
    "tar_file_key = 'path/to/your/tar/file.tar'\n",
    "\n",
    "# Fetch the .tar file from the source S3 bucket\n",
    "s3.download_file(source_bucket_name, tar_file_key, 'file.tar')\n",
    "\n",
    "# Uncompress the .tar file\n",
    "with tarfile.open('file.tar', 'r') as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "# Iterate over the extracted files\n",
    "for root, dirs, files in os.walk('.'):\n",
    "    for file in files:\n",
    "        if file.endswith('.bz2'):\n",
    "            # Uncompress each .bz2 file\n",
    "            with bz2.open(os.path.join(root, file), 'rb') as bz_file:\n",
    "                uncompressed_data = bz_file.read()\n",
    "                # Process the uncompressed data (e.g., write to a new file, parse JSON, etc.)\n",
    "                # Your processing logic goes here\n",
    "                \n",
    "                # Get the relative path of the file within the directory tree\n",
    "                relative_path = os.path.relpath(os.path.join(root, file), '.')\n",
    "\n",
    "                # Upload the processed file to the destination S3 bucket with the same directory tree structure\n",
    "                destination_key = os.path.join(relative_path, file)\n",
    "                s3.put_object(Body=uncompressed_data, Bucket=destination_bucket_name, Key=destination_key)\n",
    "\n",
    "# Terminate the EMR cluster\n",
    "emr.terminate_job_flows(JobFlowIds=[job_flow_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c38fc-7d72-4db9-9aca-13e70dfd386e",
   "metadata": {},
   "source": [
    "## Sample code to parse the twee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25fb2d0-2f9a-4ab4-acf2-64d450488311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tweet Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read the tweets file from S3 and create a dataframe\n",
    "tweets_df = spark.read.json(\"s3://your-bucket/tweets.json\")\n",
    "\n",
    "# Filter the dataframe to extract the desired data\n",
    "filtered_df = tweets_df.filter(tweets_df['text'].contains(\"your-filter-keyword\"))\n",
    "\n",
    "# Show the filtered dataframe\n",
    "filtered_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
